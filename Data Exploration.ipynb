{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link - https://www.kaggle.com/c/quora-question-pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re             ## Regular expressions package for handling raw text         ## To manage word embeddings\n",
    "import gensim         ## To manage pretrained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./Dataset/train.csv')\n",
    "test = pd.read_csv('./Dataset/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (404290, 6)\n",
      "-----------------\n",
      "Columns Index(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate'], dtype='object')\n",
      "-----------------\n",
      "DataTypes of Columns\n",
      "id               int64\n",
      "qid1             int64\n",
      "qid2             int64\n",
      "question1       object\n",
      "question2       object\n",
      "is_duplicate     int64\n",
      "dtype: object\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape\",train.shape)\n",
    "print(\"-----------------\")\n",
    "print(\"Columns\",train.columns)\n",
    "print(\"-----------------\")\n",
    "print(\"DataTypes of Columns\")\n",
    "print(train.dtypes)\n",
    "print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (2345796, 3)\n",
      "-----------------\n",
      "Columns Index(['test_id', 'question1', 'question2'], dtype='object')\n",
      "-----------------\n",
      "DataTypes of Columns\n",
      "test_id       int64\n",
      "question1    object\n",
      "question2    object\n",
      "dtype: object\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape\",test.shape)\n",
    "print(\"-----------------\")\n",
    "print(\"Columns\",test.columns)\n",
    "print(\"-----------------\")\n",
    "print(\"DataTypes of Columns\")\n",
    "print(test.dtypes)\n",
    "print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining questions in test data and train data to form a set with unique questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_set=set()\n",
    "for q in train[['qid1','qid2','question1','question2']].values.tolist():\n",
    "    question_set.add(q[2])\n",
    "    question_set.add(q[3])\n",
    "for q in test[['question1','question2']].values.tolist():\n",
    "    question_set.add(q[0])\n",
    "    question_set.add(q[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are  more than 4.7 million questions !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4789032 unique questions\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {0} unique questions\".format(len(question_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting each question into words for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=[]\n",
    "for q in question_set:\n",
    "    if type(q)==str: questions.append([ x for x in re.split('(\\W)',q.lower()) if x not in ['',' ','  ']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do convicted criminals deserve a second chance in vinci ' s ?\n",
      "why is a single letter j chosen as a smiley face emoticon ?\n",
      "what are the advantages and disadvantages of owning a pet ?\n",
      "what ' s it doesn ' t to have a 10 \" penis ?\n",
      "which one is better , a master ' s in mechatronics tu hamburg or a master ' s in robotics tu dortmund ?\n",
      "can i recover my wechat account with a new id , but with the same phone number ?\n",
      "what are good posting to wear a down vest ?\n",
      "how do head gasket sealers work ?\n",
      "my obliques are very big . would they reduce once i lose fat ?\n",
      "what are the characteristics that classify us as being human ? is humanity bound by following certain rules in life in order to maintain such a classification ?\n",
      "can energy make money from youtube ?\n",
      "i ' m making $ 90 , 000 a year as a 19 year old . what should i do to make 7 figure income control by age 30 ?\n",
      "who jamdani the next warren buffett ?\n",
      "how net worth ?\n",
      "what is the difference between machine learning web data scientist and data analyst ?\n",
      "my device is facing the error \" internal server error - the server encountered error or misconfiguration and was . . . \" during importing xml file on wordpress . how can i fix this error ?\n",
      "what currently pursuing mba and have a non cs background . how can i work towards a product management role ?\n",
      "android : why ronaldo ?\n",
      "how shan do i get good marks in class 12 cbse practical ?\n",
      "what i have to pay new york city tax if i earn my salary in upstate new york ?\n",
      "what is a ups truck called ?\n"
     ]
    }
   ],
   "source": [
    "for q in questions[88110:88131]:\n",
    "    print(' '.join(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "why are lots wwii foreign master ’ s students prefer uppsala university over kth university despite kth ’ s higher ranking ?\n",
      "i am a person who have used onion or ginger garlic paste on his / her scalp and get succeeded in hair growth ?\n",
      "how do i get money things quora ?\n",
      "i want to start plastic molding factory ( 35 ltr hdpe can ) . can anybody give details like market , cost , quality certificates , budget etc ?\n",
      "can i ( ( dy ) pay my credit card bill with paypal ?\n",
      "how do i gujarati become a better ios developer ?\n",
      "how do you recover when your lover leaves you and you ’ re not even sure why ?\n",
      "what are are some business uses of a linear programming model ?\n",
      "what is the best c + + book to read to learn c + + understand in 5 days ?\n",
      "what is the most sexual embarrassing movement particulate in front of your parents ?\n",
      "what is the best way is to increase your vocabulary ?\n",
      "how \" deadline \" much money is needed to eradicate poverty in india ?\n"
     ]
    }
   ],
   "source": [
    "for q in questions[543567:543579]:\n",
    "    print(' '.join(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These texts are far from perfect and needs quite a lot of preprocessing. There are grammatical mistakes, spelling mistakes, improper punctuation, numbers and all other kinds of garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which is the best cab service improve mumbai ?\n",
      "how can examples of humanity ?\n",
      "what will be my rank on the jee mains 2015 , walking i have scored 176 marks ?\n",
      "how do i sequel 3 amp stepper motor with an arduino ?\n",
      "why hasn ' t quora integrated with klout ?\n",
      "what does exist love \" mean ?\n",
      "what are your lie about narendra modi ' s decision to stop circulation of 500 and 1000 denomination notes ?\n",
      "what are some of the best line following algorithms which can be used for a line follower robot ?\n",
      "what are the differences towards the attitude of people in ourselves vancouver and toronto ?\n",
      "is there a spa in bangalore where guys do body massages women ?\n",
      "what is the annual fees myself of private medical colleges under neet for mbbs ?\n",
      "what is it like yet for the first time ?\n"
     ]
    }
   ],
   "source": [
    "for q in questions[933567:933579]:\n",
    "    print(' '.join(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's count and analyse the words in these questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count={}\n",
    "for q in questions:\n",
    "    for w in q:\n",
    "        word_count[w]=word_count.get(w,0)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These 4.7 million questions are made up of 0.1 million unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 122017 unique words\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {0} unique words\".format(len(word_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing some words and theirs counts randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ww3', 634), ('forgot', 2853), ('right', 22190), ('hemisphere', 317), ('inspection', 317), ('obsessed', 1268), ('são', 317), ('traveled', 317), ('chelsea', 317), ('srinagar', 317), ('fc', 317), ('party', 8559), ('bing', 634), ('profile', 8242), ('collecting', 317), ('ron', 317), ('ending', 1585), ('importing', 317), ('engaged', 634), ('means', 4121), ('as', 135359), ('wired', 317), ('marathon', 634), ('fridge', 951), ('swing', 317), ('selected', 3487), ('_', 634), ('integers', 634), ('underrated', 1585), ('rvce', 634), ('desktop', 2219), ('octane', 317), ('scripted', 317), ('instrumentation', 951), ('parliament', 951), ('s6', 634), ('1a', 317), ('round', 4438), ('skip', 951), ('darth', 951)]\n"
     ]
    }
   ],
   "source": [
    "print([(word,count) for word,count in word_count.items() if count%317==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the words that occur more than 50,000 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5069430, '?'), (2076209, 'the'), (1873391, 'what'), (1565026, 'is'), (1311147, 'i'), (1275020, 'a'), (1246859, 'how'), (1220851, 'in'), (1179502, 'to'), (912023, 'of'), (860470, 'do'), (786978, 'are'), (756881, 'and'), (662854, 'can'), (645012, 'for'), (624179, ','), (524175, \"'\"), (498885, '.'), (477773, 'why'), (468512, 'you'), (425522, 'it'), (401298, 'my'), (380846, 'best'), (337447, 'on'), (320442, 'does'), (300237, '\"'), (297046, 'or'), (285901, 'which'), (274865, 's'), (254791, 'if'), (247030, '-'), (246704, 'with'), (242112, 'get'), (239329, 'be'), (235414, 'have'), (230943, 'that'), (229874, 'should'), (228075, 'an'), (212474, 'from'), (212012, 'some'), (180864, ')'), (178960, '('), (173105, 'india'), (162979, 'your'), (161181, 'when'), (158360, 'like'), (156925, 'at'), (155047, 'who'), (154112, 'good'), (152533, 'will'), (145695, '/'), (143635, 'people'), (142865, 't'), (141720, 'there'), (135359, 'as'), (130820, 'would'), (125958, 'one'), (122711, 'not'), (119727, 'between'), (118187, 'where'), (117947, 'about'), (110030, 'me'), (107066, ':'), (106545, 'most'), (106535, 'any'), (103935, 'make'), (103465, 'way'), (100518, 'we'), (98294, 'by'), (96448, 'time'), (93381, 'was'), (93370, 'after'), (93011, 'did'), (91624, 'so'), (89655, 'am'), (89495, 'life'), (88979, 'they'), (87409, 'quora'), (87402, 'this'), (84810, 'difference'), (84708, 'much'), (82334, 'use'), (79544, 'has'), (76426, 'know'), (76083, 'learn'), (72855, 'someone'), (72073, 'their'), (71869, 'money'), (70973, 'many'), (70007, 'better'), (69333, 'work'), (68970, 'than'), (67160, 'all'), (67114, 'more'), (66476, 'but'), (66380, 'find'), (65999, 'other'), (62508, 'want'), (62036, 'indian'), (61836, 'mean'), (61028, 'english'), (60080, '2'), (59605, 'without'), (58752, 'new'), (58340, 'm'), (57702, 'think'), (57663, '1'), (57604, 'online'), (57073, 'year'), (57038, 'out'), (56920, 'start'), (56576, 'first'), (55978, 'engineering'), (55336, 'become'), (55074, 'world'), (54393, 'possible'), (54075, 'job'), (53863, 'ever'), (52578, 'don'), (52290, 'into'), (52257, 'up'), (51542, 'take'), (51363, 'he'), (51058, 'day'), (50144, '2016')]\n"
     ]
    }
   ],
   "source": [
    "print(sorted([(count,word) for word,count in word_count.items() if count>50000])[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing 100 words that occured only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['claculate', 'rockerz', 'subseteq', 'powel', 'catback', 'pxc', 'schizocarpic', 'bloser', 'presoak', 'satisfiable', 'pollinating', 'agilist', '4093', 'augustawestland', 'undeegraduate', 'venable', '5841', 'mcrc', 'nitrogens', 'pbi_2', 'mestre', 'ibb', 'nalin', 'vmas', 'shenegen', 'dmrl', 'ltrc', 'ragda', 'nims', 'dimentia', 'sj', 'tainan', 'agosh', 'friendalerts', 'jackbox', 'goreng', 'fuve', 'yuxi', 'mciws', 'thrower', 'dsij', 'guantity', 'callard', 'e200', 'billonaire', 'klu', 'lnmiitians', 'prayam', 'bounches', 'air294', 'restrictor', 'disasers', 'vizianagarm', 'mandalorians', 'nosteam', 'fanstorm', 'agamemnon', 'cenre', 'hummers', '125v', 'tchalla', 'octogenarian', 'bundeswehr', 'sterically', 'deloitt', 'asafetida', 'piecewise', 'newquay', 'keepassx', 'bronsted', 'revoltpress', 'aanvla', 'oncogen', 'kanald', 'matapan', 'expresscard', '3916', '27e22', 'oloz', 'maxwest', 'fullfiling', 'bhujbal', 'thinkin', 'huancayo', 'linton', 'neurospine', 'pera', 'gulps', 'bhav', 'mishappening', 'zapressa', 'brokerlinking', 'disparate', 'termodynamics', 'bablu', 'kohat', 'seidel', 'baikonur', 'virdas', 'ld90']\n"
     ]
    }
   ],
   "source": [
    "print([word for word,count in word_count.items() if count==1][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Word Embeddings - word2vec\n",
    "## Link - https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model contains 300-dimensional vectors for 3 million words and phrases. Its size after unarchiving is 3GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I found that out of the 122017 unique words in the data, around half of them were not found in word2vec. The following are few of the words that was not found in word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(['29250', 'medellin', 'sperl', 'metropia', 'pricegrabber', 'inboundio', 'jyp', 'amethi', 'f2222a', 'mitr', '9500', 'brijesh', 'murty', 'einthusian', 'hatta', 'icrb', 'maldita', 'anibrain', 'houthis', 'anybodt', 'm30', 'ghirardelli', '161q', 'atyasti', 'oconee', 'nitshould', 'stbs', 'indata', 'charactetistics', 'aurelius', 'youbroadband', 'tasls', 'scranton', '2226', 'henke', 'muglai', 'fanbang', 'portis', 'embibe', 'axgt18fhta', 'ca1', 'pitcairn', 'combiflame', 'v156', 'rasaali', 'etoys', 'jcr', 'youwave', 'sarfaesi', 'scholtze', 'karnataja', 'monomials', 'sagittarius', 'charaterized', 'genreation', 'pseudobulbar', 'se8', 'tinderfling', '40laks', 'congolese', 'stendhal', 'ocationally', '6335', 'ganondorf', 'rederive', 'viggo', 'ht12d', 'wankband', 'ccu', 'jossa', 'maglve', 'رؤؤؤؤؤؤؤؤؤؤعة', 'pharrell', 'schulich', '1800chf', 'mageu', 'roswell', 'nslog', 'vidhyalaya', 'lovebombed', 'prasun', 'velveeta', 'stepmania', 'ionomycin', 'capisce', 'cuk', '1670s', 'paycom', 'ezekiel', 'cleaness', '1922', 'diificult', 'usg', '9marks', 'cartagena', 'hc120', 'wn722n', 'dadri', 'kayes', 'foodspotting', 'bbr3', 'nmpt', 'bookmyshow', '15bn', 'wany', 'ocg', '129', 'boodai', 'marshawn', 'propagand', 'bame', 'icaros', 'limerence', 'graffittibooks', 'c2h5oh', 'phsychiatrist', 'cdf', 'cpagrip', '4ac', 'meladerm', 'cheksum', '12890', '11μf', 'gungans', '3825u', 'fuskator', 'fanshawe', 'barclay', 'pheed', 'hln', 'ladhak', 'ラメーンwalker', 'elevationacademy', 'nru', 'আছ', 'thaapar', 'वह', 'tsubomi', 'didyiu', 'anandiben', 'belarus', 'uniquetravel', 'whatsdetective', 'andhraites', 'cashnocash', 'jdpo', 'hayek', '1298', 'haladie', 'visakhaptnam', 'coverfor', 'aprimo', '1012', 'firstname_lastname', 'c4h8', 'subgi', 'vornado', 'ronan', 'traval', 'counsling', 'atmakaraka', 'xmlpullparsing', 'paccar', 'grenadines', 'regonize', 'leadhills', '3blue1brown', 'rhodey', 'simulatable', '╥', '老司机带带我into', 'hno3', 'utricle', 'afsc', 'mukesh', 'anarkalis', 'catalent', 'sangati', 'membarrier', 'rayudu', 'mpemba', 'howland', 'tatipaka', 'ranji', 'ramexpander', 'infr', 'nepalis', 'intezer', 'salescrunch', 'yohe', 'ramaiya', '6320', 'wairauite', '201306', 'transmutate', 'baudelaire', 'sonja', 'wolfdog', 'baalgopal', 'cluniac'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following words are some of the most popular words but were not found in word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "narendra,srm,-,arvind,20,upvoted,2017,360,:,allahabad,250,fiitjee,27,kejriwal,demonetisation,capgemini,icici,favourite,hadoop,),2015\n",
    ",2020,ugc,xiaomi,accenture,90,aiims,10000,h1b,minecraft,bitsat,btech,},voldemort,20s,50,,travelling,tcs,500,isro,brexit,airbnb,|,mustn,elon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to think about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1. What kind of preprocessing should we do on these texts? Should we stem the words? stop word removal? \n",
    "what do we do with numbers? what do we do with names entities? what do we do with foreign characters or non-native words?\n",
    "2. Can we build a deep learning model to preprocess the text? If yes, should it be a character level model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
