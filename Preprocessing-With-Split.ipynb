{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import SnowballStemmer,WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tag_map = defaultdict(lambda : \"n\")\n",
    "tag_map['J'],tag_map['V'],tag_map['R'] = \"a\",\"v\",\"r\"\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "replacements=[(r\"\\b([A-Za-z]+)'s\\b\", '\\\\1 is'),(r\"\\b([A-Za-z]+)'re\\b\", '\\\\1 are'),\n",
    "              (r\"\\b([A-Za-z]+)'ve\\b\", '\\\\1 have'),(r\"\\b([A-Za-z]+)'ll\\b\", '\\\\1 will'),\n",
    "              (r\"\\b([A-Za-z]+)n't\\b\", '\\\\1 not'),\n",
    "              (\"whats\",\"what is\"),(\"whos\",\"who is\"),(\"wheres\",\"where is\"),\n",
    "              (\"whens\",\"when is\"),(\"hows\",\"how is\"),(\" im \",\"i am\"),\n",
    "              (\"hes\",\"he is\"),(\"shes\",\"she is\"),(\"thats\",\"that is\"),\n",
    "              (\"theres\",\"there is\"),(\"isnt\",\"is not\"),(\"wasnt\",\"was not\"),\n",
    "              (\"arent\",\"are not\"),(\"werent\",\"were not\"),(\"cant\",\"can not\"),\n",
    "              (\"cannot\",\"can not\"),(\"couldnt\",\"could not\"),(\"dont\",\"do not\"),\n",
    "              (\"didnt\",\"did not\"),(\"shouldnt\",\"should not\"),(\"wouldnt\",\"would not\"),\n",
    "              (\"doesnt\",\"does not\"),(\"havent\",\"have not\"),(\"hasnt\",\"has not\"),\n",
    "              (\"hadnt\",\"had not\"),('\\s+',' '), # replace multi space with one single space\n",
    "              (\" J K \", \" JK \"),(\"banglore\", \"Banglore\"),(\"bangalore\", \"Banglore\"),(\"bengaluru\", \"Banglore\"),\n",
    "              (\"Find\", \"find\"), (\"Method\", \"method\"),(\"Astrology\", \"astrology\"),\n",
    "              (\"bestfriend\", \"best friend\"),(\" bf \",\"boy friend\"),(\" gf \",\" girl friend \"),\n",
    "              (\"boyfriend\",\" boy friend \"),(\"girlfriend\",\"girl friend\"),\n",
    "              (\"programing\", \"programming\"),(\"calender\", \"calendar\"),(\"intially\", \"initially\"),\n",
    "              (\"quikly\", \"quickly\"),(\"imrovement\", \"improvement\"),(\"demonitization\", \"demonetization\"),\n",
    "              (\" dms \", \"direct messages \"),(\"upvote\", \"up vote\"),(\" downvotes \", \" up votes \"),\n",
    "              (\"ios\", \"operating system\"),(\" iPhone \", \" phone \"),(\" iphone \", \" phone \"),\n",
    "              (\" i phone \", \" phone \"),(\" cs \", \" computer science \"),(\" cse \", \" computer science \"),\n",
    "              (\" CS \", \" computer science \"),(\" CSE \", \" computer science \"),\n",
    "              (\"KMs\", \" kilometers \"),(\"kms\", \" kilometers \"),(\"actived\", \"active\"),\n",
    "              (\" UK \", \" England \"),(\" uk \", \" England \"),(\" u s \", \" America \"),(\" USA \", \" America \"),\n",
    "              (\" US \",\" America \"),(\"the US\", \"America\"),(\" usa \", \" America \"),\n",
    "              (\"e-mail\", \"email\"),(\" 9 11 \", \"911\"),(\" b g \", \" bg \"),(\"60k\", \" 60000\"),\n",
    "              ('â‚¹',' rupee '), (' txt ',\" text \"),(\" OS \",\" operating system \"), (\"Wi-Fi\", \"wifi\"),\n",
    "              (\"cgpa\",\"gpa\"),(\"watsapp\",\"whatsapp\"),(\"tution\", \"tuition\"),\n",
    "              (\" II \", \" two \"),(\" III \", \" three \"),(\" V \", \" five \"),\n",
    "              (\"1st\",\" one \"),(\"2nd\",\" two \"),(\"3rd\",\" three \"),(\"4th\",\" four \"),(\" 10th \",\" ten \"),\n",
    "              (\" 12th \",\" twelve \"),(\" 00 \",\" 0 \"),(\" 000 \",\" 0 \"),(\" 0000 \",\" 0 \"),(\" 0 \",\" zero \"),\n",
    "              (\" 1 \",\" one \"),(\" 01 \",\" one \"),(\" 2 \",\" two \"),(\" 3 \",\" three \"),(\" 4 \",\" four \"),\n",
    "              (\" 10 \",\" ten \"),(\" 20 \",\" twenty \"),(\" 50 \",\" fifty \"),(\" 100 \",\" hundred \"),\n",
    "              (\" 1000 \",\" thousand \"),(r\"\\0rs \", \" rs \"),(r\"\\'s\", \" \"),(r\"\\'ve\", \" have \"),\n",
    "              (r\"\\'d\", \" would \"),(r\"\\'ll\", \" will \"),(r\"\\0s\", \"0\"),(r\"\\s{2,}\", \" \"),(r\"[^A-Za-z0-9]\", \" \")\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, to_lowercase=True, remove_stop_words=False, lemmatize=True, stem_words=False):\n",
    "    ## Replace old patterns with new\n",
    "    for old,new in replacements:\n",
    "        text= re.sub(old,new, text)   \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])   \n",
    "    # Convert to lowercase\n",
    "    if to_lowercase:\n",
    "        text=text.lower()\n",
    "    text = text.split()\n",
    "    # Lemmatize words\n",
    "    if lemmatize:\n",
    "        text = [ lemmatizer.lemmatize(word,tag_map[tag[0]]) for word,tag in pos_tag(text) ]  \n",
    "    # Remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = [w for w in text if not w in stop_words] \n",
    "    # Shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(text)\n",
    "    # Return the clean text as string\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned-train.csv').fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 done.\n",
      "Question 2 done.\n"
     ]
    }
   ],
   "source": [
    "# data['question1']=data['question1'].apply(lambda x:clean_text(x,True,True,True,True))\n",
    "# print('Question 1 done.')\n",
    "# data['question2']=data['question2'].apply(lambda x:clean_text(x,True,True,True,True))\n",
    "# print('Question 2 done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save The Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv('cleaned-train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Questions with QID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1, q2 = data[['qid1', 'question1']], data[['qid2', 'question2']]\n",
    "q1.columns = ['qid', 'question']\n",
    "q2.columns = ['qid', 'question']\n",
    "question_data = pd.concat((q1, q2), axis=0).fillna(\"\").sort_values(by='qid').drop_duplicates('qid').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(data, test_size=0.75, random_state=10, stratify=data[['is_duplicate']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Train Questions with QID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1, q2 = X_train[['qid1', 'question1']], X_train[['qid2', 'question2']]\n",
    "q1.columns = ['qid', 'question']\n",
    "q2.columns = ['qid', 'question']\n",
    "train_question_data = pd.concat((q1, q2), axis=0).fillna(\"\").sort_values(by='qid').drop_duplicates('qid').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = []\n",
    "# for i in range(128, 8197, 128):\n",
    "#     vectorizer = TfidfVectorizer(stop_words='english', strip_accents = 'unicode',\n",
    "#                                  max_features = i, norm='l1')\n",
    "#     x = vectorizer.fit_transform(question_data[:,1]).todense()\n",
    "#     vals.append(np.median(np.count_nonzero(x, axis=1), axis=0)[0,0])\n",
    "vectorizer = TfidfVectorizer(stop_words='english', strip_accents = 'unicode', max_features = 256, norm='l1')\n",
    "vectorizer = vectorizer.fit(train_question_data[:,1])\n",
    "q_vectors = vectorizer.transform(question_data[:,1]).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization of Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = X_train.values\n",
    "train_vectors = np.zeros((np_data.shape[0], q_vectors.shape[1]+1))\n",
    "for i in range(np_data.shape[0]):\n",
    "    train_vectors[i,:-1] = q_vectors[np_data[i,1]-1] - q_vectors[np_data[i,2]-1]\n",
    "    train_vectors[i,-1] = np_data[i,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = X_test.values\n",
    "test_vectors = np.zeros((np_data.shape[0], q_vectors.shape[1]+1))\n",
    "for i in range(np_data.shape[0]):\n",
    "    test_vectors[i,:-1] = q_vectors[np_data[i,1]-1] - q_vectors[np_data[i,2]-1]\n",
    "    test_vectors[i,-1] = np_data[i,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=8, random_state=10)\n",
    "pca = pca.fit(train_vectors[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca = pca.transform(train_vectors[:,:-1])\n",
    "train_pca = np.hstack((train_pca, train_vectors[:,-1:]))\n",
    "test_pca = pca.transform(test_vectors[:,:-1])\n",
    "test_pca = np.hstack((test_pca, test_vectors[:,-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['feature {}'.format(x+1) for x in range(8)]\n",
    "column_names.append('is_duplicate')\n",
    "train_data = pd.DataFrame(data=train_pca, columns=column_names).astype({'is_duplicate': int})\n",
    "test_data = pd.DataFrame(data=test_pca, columns=column_names).astype({'is_duplicate': int})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('pca_train_2.csv', index=False)\n",
    "test_data.to_csv('pca_test_2.csv', index=False)\n",
    "pickle.dump(pca, open('pca_obj_2.pkl', 'wb'))\n",
    "pickle.dump(vectorizer, open('tf-idf_obj_2', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
