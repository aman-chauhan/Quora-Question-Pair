{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reference: https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
    "##Reference: https://www.kaggle.com/life2short/data-processing-replace-abbreviation-of-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer,WordNetLemmatizer\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into train and test set and saving them in seperate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Dataset/train.csv\")\n",
    "df = df.fillna('empty') ## If any of the question is na, replace it with \"empty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tag_map = defaultdict(lambda : \"n\")\n",
    "tag_map['J'],tag_map['V'],tag_map['R'] = \"a\",\"v\",\"r\"\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "replacements=[(r\"\\b([A-Za-z]+)'s\\b\", '\\\\1 is'),(r\"\\b([A-Za-z]+)'re\\b\", '\\\\1 are'),\n",
    "              (r\"\\b([A-Za-z]+)'ve\\b\", '\\\\1 have'),(r\"\\b([A-Za-z]+)'ll\\b\", '\\\\1 will'),\n",
    "              (r\"\\b([A-Za-z]+)n't\\b\", '\\\\1 not'),\n",
    "              (\"whats\",\"what is\"),(\"whos\",\"who is\"),(\"wheres\",\"where is\"),\n",
    "              (\"whens\",\"when is\"),(\"hows\",\"how is\"),(\" im \",\"i am\"),\n",
    "              (\"hes\",\"he is\"),(\"shes\",\"she is\"),(\"thats\",\"that is\"),\n",
    "              (\"theres\",\"there is\"),(\"isnt\",\"is not\"),(\"wasnt\",\"was not\"),\n",
    "              (\"arent\",\"are not\"),(\"werent\",\"were not\"),(\"cant\",\"can not\"),\n",
    "              (\"cannot\",\"can not\"),(\"couldnt\",\"could not\"),(\"dont\",\"do not\"),\n",
    "              (\"didnt\",\"did not\"),(\"shouldnt\",\"should not\"),(\"wouldnt\",\"would not\"),\n",
    "              (\"doesnt\",\"does not\"),(\"havent\",\"have not\"),(\"hasnt\",\"has not\"),\n",
    "              (\"hadnt\",\"had not\"),\n",
    "              ('\\s+',' '), # replace multi space with one single space\n",
    "              (\" J K \", \" JK \"),(\"banglore\", \"Banglore\"),(\"bangalore\", \"Banglore\"),(\"bengaluru\", \"Banglore\"),\n",
    "              (\"Find\", \"find\"), (\"Method\", \"method\"),(\"Astrology\", \"astrology\"),\n",
    "              (\"bestfriend\", \"best friend\"),(\" bf \",\"boy friend\"),(\" gf \",\" girl friend \"),\n",
    "              (\"boyfriend\",\" boy friend \"),(\"girlfriend\",\"girl friend\"),\n",
    "              (\"programing\", \"programming\"),(\"calender\", \"calendar\"),(\"intially\", \"initially\"), (\"quikly\", \"quickly\"),\n",
    "              (\"imrovement\", \"improvement\"),(\"demonitization\", \"demonetization\"),(\" dms \", \"direct messages \"),\n",
    "              (\"upvote\", \"up vote\"),(\" downvotes \", \" up votes \"),\n",
    "              (\"ios\", \"operating system\"),(\" iPhone \", \" phone \"),(\" iphone \", \" phone \"),(\" i phone \", \" phone \"),\n",
    "              (\" cs \", \" computer science \"),(\" cse \", \" computer science \"),(\" CS \", \" computer science \"),\n",
    "              (\" CSE \", \" computer science \"),\n",
    "              (\"KMs\", \" kilometers \"),(\"kms\", \" kilometers \"),(\"actived\", \"active\"),\n",
    "              (\" UK \", \" England \"),(\" uk \", \" England \"),(\" u s \", \" America \"),(\" USA \", \" America \"),\n",
    "              (\" US \",\" America \"),(\"the US\", \"America\"),(\" usa \", \" America \"),\n",
    "              (\"e-mail\", \"email\"),(\" 9 11 \", \"911\"),(\" b g \", \" bg \"),(\"60k\", \" 60000\"),\n",
    "              ('â‚¹',' rupee '), (' txt ',\" text \"),(\" OS \",\" operating system \"), (\"Wi-Fi\", \"wifi\"),\n",
    "              (\"cgpa\",\"gpa\"),(\"watsapp\",\"whatsapp\"),(\"tution\", \"tuition\"),\n",
    "              (\" II \", \" two \"),(\" III \", \" three \"),(\" V \", \" five \"),\n",
    "              (\"1st\",\" one \"),(\"2nd\",\" two \"),(\"3rd\",\" three \"),(\"4th\",\" four \"),(\" 10th \",\" ten \"),(\" 12th \",\" twelve \"),\n",
    "              (\" 00 \",\" 0 \"),(\" 000 \",\" 0 \"),(\" 0000 \",\" 0 \"),(\" 0 \",\" zero \"),\n",
    "              (\" 1 \",\" one \"),(\" 01 \",\" one \"),(\" 2 \",\" two \"),(\" 3 \",\" three \"),(\" 4 \",\" four \"),\n",
    "              (\" 10 \",\" ten \"),(\" 20 \",\" twenty \"),(\" 50 \",\" fifty \"),(\" 100 \",\" hundred \"),(\" 1000 \",\" thousand \"),\n",
    "              (r\"\\0rs \", \" rs \"),(r\"\\'s\", \" \"),(r\"\\'ve\", \" have \"),(r\"\\'d\", \" would \"),(r\"\\'ll\", \" will \"),\n",
    "              (r\"\\0s\", \"0\"),(r\"\\s{2,}\", \" \"),(r\"[^A-Za-z0-9]\", \" \")\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, to_lowercase=True, remove_stop_words=False, lemmatize=True, stem_words=False):\n",
    "    \n",
    "    ## Replace old patterns with new\n",
    "    for old,new in replacements:\n",
    "        text= re.sub(old,new, text)\n",
    "        \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])   \n",
    "    \n",
    "    # Convert to lowercase\n",
    "    if to_lowercase:\n",
    "        text=text.lower()\n",
    "\n",
    "    text = text.split()\n",
    "    \n",
    "    # Lemmatize words\n",
    "    if lemmatize:\n",
    "        text = [ lemmatizer.lemmatize(word,tag_map[tag[0]]) for word,tag in pos_tag(text) ]\n",
    "        \n",
    "    # Remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = [w for w in text if not w in stop_words] \n",
    "    \n",
    "    # Shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = [stemmer.stem(word) for word in text]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Return the clean text as string\n",
    "    return(text)\n",
    "\n",
    "## clean_text(\"He swam and ran stupidly before being caught\",True,True,True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Will take some time to run, close to an hour\n",
    "df['question1']=df['question1'].apply(lambda x:clean_text(x,True,True,True,False))\n",
    "df['question2']=df['question2'].apply(lambda x:clean_text(x,True,True,True,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 270000 \n",
    "for i in range(a,a+20):\n",
    "    print(clean_text(df.question1[i],True,True,True,False))\n",
    "    print(clean_text(df.question2[i],True,True,True,False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = df[['question1', 'question2']], df['is_duplicate']\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.20, random_state=777, stratify=y)\n",
    "X_train['is_duplicate']=y_train\n",
    "X_train.to_csv('train.csv', index=False)\n",
    "X_test['is_duplicate']=y_test\n",
    "X_test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv',encoding = \"ISO-8859-1\")\n",
    "df_test = pd.read_csv('test.csv',encoding = \"ISO-8859-1\")\n",
    "X_train, y_train  = df_train[['question1', 'question2']], df_train['is_duplicate']\n",
    "X_test, y_test  = df_test[['question1', 'question2']], df_test['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80858, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    204022\n",
       "1    119410\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    51005\n",
       "1    29853\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
